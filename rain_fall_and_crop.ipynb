{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d7dbd324c1235",
   "metadata": {},
   "source": [
    "# Part 1: Rainfall Data Engineering\n",
    "\n",
    "# Step 1: Initial Structure Inspection\n",
    "\n",
    "**Objective:**\n",
    "Before processing the entire batch of 149 files, we must understand the structure of the data by inspecting a single sample file. This step prevents blind merging errors caused by mismatched headers or data type inconsistencies.\n",
    "\n",
    "**Methodology:**\n",
    "\n",
    "1. **Locate Files:** Use the `glob` library to find all `.xlsx` files in the source directory.\n",
    "2. **Load Sample:** Read the first available file into a pandas DataFrame.\n",
    "3. **Inspect Schema:**\n",
    "   - **Column Headers:** Check naming conventions and spacing (e.g., `'Station Name'`).\n",
    "   - **Data Types:** Verify that numeric fields like `Year` and `Rainfall (mm)` are not stored as text.\n",
    "   - **Data Preview:** Display the first 5 rows to understand structure and formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "333b3a8fa134b338",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T16:45:20.184851Z",
     "start_time": "2026-02-08T16:45:18.293958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inspecting: Monthly Rainfall in (2014)_01.xlsx ---\n",
      "\n",
      "1. Column Headers:\n",
      "['Station Name', 'State', 'District', 'Month', 'Year', 'Rainfall (mm)']\n",
      "\n",
      "2. First 5 rows of data:\n",
      "                 Station Name       State         District Month    Year  \\\n",
      "0                      PILANI   Rajasthan           PILANI   Jan  2014.0   \n",
      "1          MANGALORE BAJPE(A)   Karnataka  DAKSHIN KANNADA   Jan  2014.0   \n",
      "2                    JHALAWAR   Rajasthan         JHALAWAR   Jan  2014.0   \n",
      "3                      TEZPUR       Assam         SONITPUR   Jan  2014.0   \n",
      "4  COIMBATORE / PEELAMEDU (A)  Tamil Nadu       COIMBATORE   Jan  2014.0   \n",
      "\n",
      "   Rainfall (mm)  \n",
      "0            0.0  \n",
      "1            0.0  \n",
      "2           29.0  \n",
      "3            0.3  \n",
      "4            0.0  \n",
      "\n",
      "3. Data Types:\n",
      "Station Name         str\n",
      "State                str\n",
      "District             str\n",
      "Month                str\n",
      "Year             float64\n",
      "Rainfall (mm)    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Path configuration\n",
    "folder_path = r\"rain_fall\"\n",
    "\n",
    "# Get a list of all Excel files\n",
    "all_files = glob.glob(os.path.join(folder_path, \"*.xlsx\"))\n",
    "\n",
    "if all_files:\n",
    "    # Read the first file found\n",
    "    first_file = all_files[0]\n",
    "    df_sample = pd.read_excel(first_file)\n",
    "\n",
    "    print(f\"--- Inspecting: {os.path.basename(first_file)} ---\")\n",
    "    print(\"\\n1. Column Headers:\")\n",
    "    print(df_sample.columns.tolist())\n",
    "\n",
    "    print(\"\\n2. First 5 rows of data:\")\n",
    "    print(df_sample.head())\n",
    "\n",
    "    print(\"\\n3. Data Types:\")\n",
    "    print(df_sample.dtypes)\n",
    "else:\n",
    "    print(\"No Excel files found in the specified directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb1bcb63870c02",
   "metadata": {},
   "source": [
    "# Step 2: Batch Loading and Data Integrity Audit\n",
    "\n",
    "**Objective:**\n",
    "Load all rainfall files (2010–2022) to detect inconsistencies, formatting errors, and unwanted metadata rows.\n",
    "\n",
    "**Methodology:**\n",
    "\n",
    "1. **Bulk Read:** Loop through all Excel files and store them in a list.\n",
    "2. **Concatenate:** Merge all files into a temporary DataFrame (`raw_df`).\n",
    "3. **Unique Value Inspection:**\n",
    "   - **States:** Detect spelling inconsistencies or trailing spaces.\n",
    "   - **Years:** Check for float representations like `2010.0`.\n",
    "   - **Months:** Validate month naming consistency.\n",
    "   - **Districts:** Verify duplicates or formatting issues.\n",
    "   - **Station Names:** Detect footer text such as \"Copyright\" or URLs.\n",
    "\n",
    "**Key Insight:**\n",
    "Footer metadata and empty rows often get imported as data and must be removed before analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "932fdf903e4509ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T16:46:24.254205Z",
     "start_time": "2026-02-08T16:46:14.185915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 149 files. Reading them now...\n",
      "\n",
      "--- Unique Value Inspection ---\n",
      "Total Rows Loaded: 36159\n",
      "\n",
      "Unique States (34):\n",
      "<StringArray>\n",
      "[                  'Rajasthan',                   'Karnataka',\n",
      "                       'Assam',                  'Tamil Nadu',\n",
      "                     'Gujarat',                 'West Bengal',\n",
      "                      'Kerala',               'Uttar Pradesh',\n",
      "                 'Lakshadweep',                      'Punjab',\n",
      "                 'Uttarakhand',              'Andhra Pradesh',\n",
      "                         'Goa',            'Himachal Pradesh',\n",
      "                      'Odisha',           'Jammu and Kashmir',\n",
      "              'Madhya Pradesh',                     'Haryana',\n",
      "                 'Maharashtra',                'Chhattisgarh',\n",
      "                       'Delhi',                       'Bihar',\n",
      "                   'Meghalaya', 'Andaman and Nicobar Islands',\n",
      "                   'Telangana',                  'Puducherry',\n",
      "                     'Tripura',           'Arunachal Pradesh',\n",
      "                      'Ladakh',                   'Jharkhand',\n",
      "                     'Manipur',                           nan,\n",
      "                      'Sikkim',                     'Mizoram',\n",
      "                    'Nagaland']\n",
      "Length: 35, dtype: str\n",
      "\n",
      "Unique Years:\n",
      "[np.float64(2014.0), np.float64(nan), np.float64(2010.0), np.float64(2011.0), np.float64(2012.0), np.float64(2013.0), np.float64(2015.0), np.float64(2016.0), np.float64(2017.0), np.float64(2018.0), np.float64(2019.0), np.float64(2020.0), np.float64(2021.0), np.float64(2022.0)]\n",
      "\n",
      "Unique Months:\n",
      "<StringArray>\n",
      "['Jan',   nan, 'May', 'Oct', 'Jul', 'Jun', 'Aug', 'Nov', 'Dec', 'Feb', 'Sep',\n",
      " 'Apr', 'Mar']\n",
      "Length: 13, dtype: str\n",
      "\n",
      "Unique Districts (329):\n",
      "<StringArray>\n",
      "[         'PILANI', 'DAKSHIN KANNADA',        'JHALAWAR',        'SONITPUR',\n",
      "      'COIMBATORE',           'SURAT',      'JALPAIGURI',       'ERNAKULAM',\n",
      "   'SURENDRANAGAR',       'RAIBARELI',\n",
      " ...\n",
      "           'SARAN',       'KANDHAMAL',      'SINDHUDURG',            'GAYA',\n",
      "       'OSMANABAD',     'TIRUNELVELI',  'WEST SINGHBHUM',      'PERAMBALUR',\n",
      "       'ALIRAJPUR',           'NADED']\n",
      "Length: 330, dtype: str\n",
      "\n",
      "Unique Station Names (408):\n",
      "<StringArray>\n",
      "[                    'PILANI',         'MANGALORE BAJPE(A)',\n",
      "                   'JHALAWAR',                     'TEZPUR',\n",
      " 'COIMBATORE / PEELAMEDU (A)',                      'SURAT',\n",
      "                 'JALPAIGURI',   'KOCHI A.P.(NEDUMBASSERY)',\n",
      "              'SURENDRANAGAR',                 'FURSATGANJ',\n",
      " ...\n",
      "                  'OSMANABAD',              'PALAYAMKOTTAI',\n",
      "                   'CHAIBASA',             'UTHAGAMANDALAM',\n",
      "                   'ARIYALUR',                  'ALIRAJPUR',\n",
      "              'LUDHIANA(PAU)',                   'SONAMARG',\n",
      "                      'NADED',                       'JEUR']\n",
      "Length: 409, dtype: str\n"
     ]
    }
   ],
   "source": [
    "folder_path = r\"rain_fall\"\n",
    "all_files = glob.glob(os.path.join(folder_path, \"*.xlsx\"))\n",
    "\n",
    "# Create an empty list to store dataframes\n",
    "df_list = []\n",
    "\n",
    "print(f\"Found {len(all_files)} files. Reading them now...\")\n",
    "\n",
    "for filename in all_files:\n",
    "    try:\n",
    "        df = pd.read_excel(filename)\n",
    "        df_list.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filename}: {e}\")\n",
    "\n",
    "# Combine into a temporary raw dataframe\n",
    "if df_list:\n",
    "    raw_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    print(\"\\n--- Unique Value Inspection ---\")\n",
    "    print(f\"Total Rows Loaded: {len(raw_df)}\")\n",
    "\n",
    "    # 1. Check States\n",
    "    print(f\"\\nUnique States ({raw_df['State'].nunique()}):\")\n",
    "    print(raw_df['State'].unique())\n",
    "\n",
    "    # 2. Check Years\n",
    "    print(f\"\\nUnique Years:\")\n",
    "    print(sorted(raw_df['Year'].unique()))\n",
    "\n",
    "    # 3. Check Months\n",
    "    print(f\"\\nUnique Months:\")\n",
    "    print(raw_df['Month'].unique())\n",
    "\n",
    "    # 4. Check Districts (Added as requested)\n",
    "    print(f\"\\nUnique Districts ({raw_df['District'].nunique()}):\")\n",
    "    print(raw_df['District'].unique())\n",
    "\n",
    "    # 5. Check Station Names\n",
    "    print(f\"\\nUnique Station Names ({raw_df['Station Name'].nunique()}):\")\n",
    "    # Note: This list might be very long\n",
    "    print(raw_df['Station Name'].unique())\n",
    "\n",
    "else:\n",
    "    print(\"No data loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4a66971243f16",
   "metadata": {},
   "source": [
    "# Step 3: Data Cleaning, Formatting, and Final Export\n",
    "\n",
    "**Objective:**\n",
    "Transform raw rainfall data into a clean master dataset suitable for analysis and storage.\n",
    "\n",
    "**Cleaning Logic Applied:**\n",
    "\n",
    "### 1. Garbage Removal\n",
    "- Convert `Year` to numeric using coercion (`errors='coerce'`).\n",
    "- Drop rows missing critical fields:\n",
    "  - `Year`\n",
    "  - `Month`\n",
    "  - `Station Name`\n",
    "\n",
    "### 2. Type Conversion\n",
    "- Convert `Year` from float to integer.\n",
    "- Convert `Rainfall (mm)` to numeric.\n",
    "\n",
    "### 3. String Sanitization\n",
    "- Remove leading and trailing whitespace from:\n",
    "  - `State`\n",
    "  - `District`\n",
    "  - `Station Name`\n",
    "  - `Month`\n",
    "\n",
    "### 4. Directory Handling\n",
    "- Create the `results` folder if it does not exist before saving output.\n",
    "\n",
    "**Output:**\n",
    "`Final_Rainfall_Data_2010_2022.xlsx`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1792276a5448ff0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T16:47:11.620950Z",
     "start_time": "2026-02-08T16:47:05.453622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. STARTING CLEANING PROCESS ---\n",
      "Removed 596 garbage/footer rows.\n",
      "\n",
      "--- Final Validation ---\n",
      "Unique Years: [np.int64(2010), np.int64(2011), np.int64(2012), np.int64(2013), np.int64(2014), np.int64(2015), np.int64(2016), np.int64(2017), np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022)]\n",
      "Garbage text successfully removed from Station Names.\n",
      "\n",
      "SUCCESS! Cleaned file saved at:\n",
      "rain_fall/results/Final_Rainfall_Data_2010_2022.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if raw_df exists from the previous step\n",
    "if 'raw_df' not in locals():\n",
    "    print(\"'raw_df' not found. Please run Code 2 again to load the files first.\")\n",
    "else:\n",
    "    print(\"--- 1. STARTING CLEANING PROCESS ---\")\n",
    "\n",
    "    # Create a copy\n",
    "    df_clean = raw_df.copy()\n",
    "\n",
    "    # A. Coerce 'Year' to Numeric\n",
    "    # This is crucial. It turns any text in the Year column (if any) into NaN\n",
    "    df_clean['Year'] = pd.to_numeric(df_clean['Year'], errors='coerce')\n",
    "\n",
    "    # B. Remove Empty Rows (The Main Filter)\n",
    "    # We drop rows where Year, Month, or Station Name is missing (NaN)\n",
    "    # This automatically removes the \"Copyright\", \"https\", and blank rows\n",
    "    rows_before = len(df_clean)\n",
    "    df_clean = df_clean.dropna(subset=['Year', 'Month', 'Station Name'])\n",
    "    rows_after = len(df_clean)\n",
    "\n",
    "    print(f\"Removed {rows_before - rows_after} garbage/footer rows.\")\n",
    "\n",
    "    # C. Fix Formatting\n",
    "    # Convert Year to integer (2010.0 -> 2010)\n",
    "    df_clean['Year'] = df_clean['Year'].astype(int)\n",
    "\n",
    "    # Coerce Rainfall to numeric (turn errors to NaN)\n",
    "    df_clean['Rainfall (mm)'] = pd.to_numeric(df_clean['Rainfall (mm)'], errors='coerce')\n",
    "\n",
    "    # Clean text columns (remove extra spaces)\n",
    "    text_cols = ['Station Name', 'State', 'District', 'Month']\n",
    "    for col in text_cols:\n",
    "        df_clean[col] = df_clean[col].astype(str).str.strip()\n",
    "\n",
    "    # --- 2. VALIDATION ---\n",
    "    print(\"\\n--- Final Validation ---\")\n",
    "    print(f\"Unique Years: {sorted(df_clean['Year'].unique())}\")\n",
    "\n",
    "    # Check if 'Copyright' or 'https' still exists in Station Name\n",
    "    bad_stations = df_clean[df_clean['Station Name'].str.contains(\"Copyright|http\", case=False)]\n",
    "    if bad_stations.empty:\n",
    "        print(\"Garbage text successfully removed from Station Names.\")\n",
    "    else:\n",
    "        print(f\"Warning: {len(bad_stations)} garbage rows remain.\")\n",
    "\n",
    "# --- 3. SAVE MERGED FILE ---\n",
    "\n",
    "# 1. Define the subfolder name\n",
    "output_folder = os.path.join(folder_path, \"results\")\n",
    "\n",
    "# 2. Check if it exists. If not, create it!\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "    print(f\"Created new folder: {output_folder}\")\n",
    "\n",
    "# 3. Define the full file path\n",
    "output_file = os.path.join(output_folder, \"Final_Rainfall_Data_2010_2022.xlsx\")\n",
    "\n",
    "try:\n",
    "    df_clean.to_excel(output_file, index=False)\n",
    "    print(f\"\\nSUCCESS! Cleaned file saved at:\\n{output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7e4dd2d3b3f1c9",
   "metadata": {},
   "source": [
    "# Step 4: OLTP Database Architecture (Normalization)\n",
    "\n",
    "**Objective:**\n",
    "Convert flat Excel storage into a normalized relational database system.\n",
    "\n",
    "**Normalization Principle (3NF):**\n",
    "Instead of repeating state and district names thousands of times, store them once and reference them using unique IDs.\n",
    "\n",
    "**Database Structure:**\n",
    "\n",
    "### Dimension Tables\n",
    "- **States** (`StateID`, `StateName`)\n",
    "- **Districts** (`DistrictID`, `DistrictName`, `StateID`)\n",
    "- **Stations** (`StationID`, `StationName`, `DistrictID`)\n",
    "\n",
    "### Fact Table\n",
    "- **Rainfall_Readings** (`StationID`, `Year`, `Month`, `Rainfall`)\n",
    "\n",
    "Foreign keys replace textual values to reduce redundancy and improve data integrity.\n",
    "\n",
    "**Output:**\n",
    "`Weather_OLTP.db`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1837486bd3c48773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T16:47:40.721188Z",
     "start_time": "2026-02-08T16:47:33.596629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BUILDING OLTP ARCHITECTURE ---\n",
      "OLTP Database created successfully.\n",
      "Verifying Table Counts:\n",
      "States: 34\n",
      "Districts: 329\n",
      "Stations: 405\n",
      "Readings: 35563\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Load the clean data\n",
    "file_path = r\"rain_fall/results/Final_Rainfall_Data_2010_2022.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Connect to SQLite database (creates a file on your disk)\n",
    "db_path = r\"rain_fall\\results\\Weather_OLTP.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(\"--- BUILDING OLTP ARCHITECTURE ---\")\n",
    "\n",
    "# 1. Create Lookup Tables (States, Districts, Stations)\n",
    "# We use .unique() to get distinct values and assign IDs\n",
    "\n",
    "# A. States Table\n",
    "states = df['State'].unique()\n",
    "df_states = pd.DataFrame(states, columns=['StateName'])\n",
    "df_states.reset_index(inplace=True)\n",
    "df_states.rename(columns={'index': 'StateID'}, inplace=True)\n",
    "df_states.to_sql('States', conn, if_exists='replace', index=False)\n",
    "\n",
    "# B. Districts Table (Needs StateID)\n",
    "# Merge original data with State IDs to get the relationship\n",
    "district_map = df[['State', 'District']].drop_duplicates()\n",
    "district_map = district_map.merge(df_states, left_on='State', right_on='StateName')\n",
    "df_districts = district_map[['District', 'StateID']].reset_index()\n",
    "df_districts.rename(columns={'index': 'DistrictID', 'District': 'DistrictName'}, inplace=True)\n",
    "df_districts.to_sql('Districts', conn, if_exists='replace', index=False)\n",
    "\n",
    "# C. Stations Table (Needs DistrictID)\n",
    "station_map = df[['District', 'Station Name']].drop_duplicates()\n",
    "station_map = station_map.merge(df_districts, left_on='District', right_on='DistrictName')\n",
    "df_stations = station_map[['Station Name', 'DistrictID']].reset_index()\n",
    "df_stations.rename(columns={'index': 'StationID', 'Station Name': 'StationName'}, inplace=True)\n",
    "df_stations.to_sql('Stations', conn, if_exists='replace', index=False)\n",
    "\n",
    "# 2. Create the Fact/Transaction Table (Rainfall Readings)\n",
    "# This replaces names with IDs to save space\n",
    "fact_df = df.merge(df_stations, left_on='Station Name', right_on='StationName')\n",
    "final_oltp = fact_df[['StationID', 'Year', 'Month', 'Rainfall (mm)']]\n",
    "final_oltp.to_sql('Rainfall_Readings', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"OLTP Database created successfully.\")\n",
    "print(\"Verifying Table Counts:\")\n",
    "print(f\"States: {pd.read_sql('SELECT COUNT(*) FROM States', conn).iloc[0,0]}\")\n",
    "print(f\"Districts: {pd.read_sql('SELECT COUNT(*) FROM Districts', conn).iloc[0,0]}\")\n",
    "print(f\"Stations: {pd.read_sql('SELECT COUNT(*) FROM Stations', conn).iloc[0,0]}\")\n",
    "print(f\"Readings: {pd.read_sql('SELECT COUNT(*) FROM Rainfall_Readings', conn).iloc[0,0]}\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f68e33b234754",
   "metadata": {},
   "source": [
    "# Step 5: OLAP Operations (Business Intelligence)\n",
    "\n",
    "**Objective:**\n",
    "Move from transactional storage (OLTP) to analytical processing (OLAP) for insight generation.\n",
    "\n",
    "**Data Enrichment:**\n",
    "A new dimension called `Season` is derived from `Month`:\n",
    "\n",
    "- **Winter:** Jan, Feb\n",
    "- **Summer:** Mar, Apr, May\n",
    "- **Monsoon:** Jun, Jul, Aug, Sep\n",
    "- **Post-Monsoon:** Oct, Nov, Dec\n",
    "\n",
    "**OLAP Operations Implemented:**\n",
    "\n",
    "1. **Roll-Up:** Aggregate monthly rainfall into yearly totals.\n",
    "2. **Drill-Down:** Break state-level summaries into district-level insights.\n",
    "3. **Slice:** Filter data for a single dimension value (e.g., `Year = 2020`).\n",
    "4. **Dice:** Filter across multiple dimensions (e.g., Kerala during Monsoon).\n",
    "5. **Pivot:** Cross-tabulate rainfall by `State` and `Year`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dd344c617d1b862",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T16:47:55.692003Z",
     "start_time": "2026-02-08T16:47:47.996232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- OLAP OPERATIONS ---\n",
      "\n",
      "[A] ROLL-UP: Total Rainfall per Year\n",
      "Year\n",
      "2010    429768.3\n",
      "2011    368934.7\n",
      "2012    379180.4\n",
      "2013    450832.3\n",
      "2014    333312.4\n",
      "Name: Rainfall (mm), dtype: float64\n",
      "\n",
      "[B] DRILL-DOWN: Rainfall in Maharashtra (By District)\n",
      "District\n",
      "RATNAGIRI         276.692523\n",
      "SATARA            251.237438\n",
      "GREATER MUMBAI    215.146429\n",
      "RAIGARH           198.770732\n",
      "THANE             196.495082\n",
      "Name: Rainfall (mm), dtype: float64\n",
      "\n",
      "[C] SLICE: Top 5 Wettest States in 2020\n",
      "State\n",
      "Meghalaya                      448.323077\n",
      "Chhattisgarh                   364.900000\n",
      "Arunachal Pradesh              310.707143\n",
      "Kerala                         241.459441\n",
      "Andaman and Nicobar Islands    217.996667\n",
      "Name: Rainfall (mm), dtype: float64\n",
      "\n",
      "[D] DICE: Kerala during Monsoon Season\n",
      "Average Monsoon Rainfall in Kerala: 436.40 mm\n",
      "\n",
      "[E] PIVOT TABLE (Cross-Tabulation)\n",
      "Year                               2010        2011        2012        2013  \\\n",
      "State                                                                         \n",
      "Andaman and Nicobar Islands  216.295833  305.987500  284.950000  297.430556   \n",
      "Andhra Pradesh               109.974112   69.048947   88.730556   85.168487   \n",
      "Arunachal Pradesh            318.836364  211.018182  343.394444  231.745714   \n",
      "Assam                        203.503571  147.020000  189.585000  162.858511   \n",
      "Bihar                         48.242857   96.504348   27.012500  115.288889   \n",
      "\n",
      "Year                               2014  \n",
      "State                                    \n",
      "Andaman and Nicobar Islands  215.195238  \n",
      "Andhra Pradesh                72.082105  \n",
      "Arunachal Pradesh            258.210714  \n",
      "Assam                        171.172917  \n",
      "Bihar                        108.200000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "file_path = r\"rain_fall/results/Final_Rainfall_Data_2010_2022.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "print(\"--- OLAP OPERATIONS ---\")\n",
    "\n",
    "# 1. ENRICHMENT (Adding Dimensions)\n",
    "# Add a 'Season' dimension for better analysis\n",
    "def get_season(month):\n",
    "    if month in ['Jun', 'Jul', 'Aug', 'Sep']: return 'Monsoon'\n",
    "    elif month in ['Oct', 'Nov', 'Dec']: return 'Post-Monsoon'\n",
    "    elif month in ['Jan', 'Feb']: return 'Winter'\n",
    "    else: return 'Summer'\n",
    "\n",
    "df['Season'] = df['Month'].apply(get_season)\n",
    "\n",
    "# --- OPERATION A: ROLL-UP (Summarize details to a higher level) ---\n",
    "# Moving from Monthly Data -> Yearly Data\n",
    "print(\"\\n[A] ROLL-UP: Total Rainfall per Year\")\n",
    "rollup = df.groupby('Year')['Rainfall (mm)'].sum()\n",
    "print(rollup.head())\n",
    "\n",
    "# --- OPERATION B: DRILL-DOWN (Break summary into details) ---\n",
    "# Breaking a specific State (Maharashtra) into Districts\n",
    "print(\"\\n[B] DRILL-DOWN: Rainfall in Maharashtra (By District)\")\n",
    "drilldown = df[df['State'] == 'Maharashtra'].groupby('District')['Rainfall (mm)'].mean().sort_values(ascending=False)\n",
    "print(drilldown.head(5))\n",
    "\n",
    "# --- OPERATION C: SLICE (Filter for one specific dimension) ---\n",
    "# Taking a single slice of the \"Cube\": Only the year 2020\n",
    "print(\"\\n[C] SLICE: Top 5 Wettest States in 2020\")\n",
    "slice_2020 = df[df['Year'] == 2020].groupby('State')['Rainfall (mm)'].mean().sort_values(ascending=False).head(5)\n",
    "print(slice_2020)\n",
    "\n",
    "# --- OPERATION D: DICE (Filter on multiple dimensions) ---\n",
    "# Sub-cube: 'Kerala' AND 'Monsoon' Season\n",
    "print(\"\\n[D] DICE: Kerala during Monsoon Season\")\n",
    "dice_df = df[(df['State'] == 'Kerala') & (df['Season'] == 'Monsoon')]\n",
    "print(f\"Average Monsoon Rainfall in Kerala: {dice_df['Rainfall (mm)'].mean():.2f} mm\")\n",
    "\n",
    "# --- OPERATION E: PIVOT (The Full OLAP Cube View) ---\n",
    "# Rows=State, Columns=Year, Values=Rainfall\n",
    "print(\"\\n[E] PIVOT TABLE (Cross-Tabulation)\")\n",
    "pivot_cube = pd.pivot_table(df, values='Rainfall (mm)', index='State', columns='Year', aggfunc='mean')\n",
    "print(pivot_cube.iloc[:5, :5]) # Showing first 5 states and first 5 years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea6096767d368bb",
   "metadata": {},
   "source": [
    "# Part 2: Crop Data Engineering\n",
    "\n",
    "# Step 6: Loading the Complex Crop Report\n",
    "\n",
    "**Objective:**\n",
    "Load the crop production dataset for processing.\n",
    "\n",
    "**Challenge:**\n",
    "The file has a `.xls` extension but is actually an HTML table saved with an Excel extension.\n",
    "\n",
    "**Solution:**\n",
    "Use `pandas.read_html()` instead of standard Excel readers to parse the file correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7956d8bd099c3b68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T16:48:12.984892Z",
     "start_time": "2026-02-08T16:48:04.478741Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read with 'xlrd' engine...\n",
      "Standard load failed: `Import xlrd` failed. Install xlrd >= 2.0.1 for xls Excel support Use pip or conda to install the xlrd package.\n",
      "\n",
      "--- ATTEMPT 2: Checking if file is actually HTML/XML (Web Export) ---\n",
      "HTML load also failed: `Import lxml` failed.  Use pip or conda to install the lxml package.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the file\n",
    "file_path = r\"Crop.xls\"\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(file_path):\n",
    "    try:\n",
    "        # ATTEMPT 1: Read as standard legacy Excel (.xls) using xlrd engine\n",
    "        print(\"Attempting to read with 'xlrd' engine...\")\n",
    "        df_crop_sample = pd.read_excel(file_path, header=[0, 1], engine='xlrd')\n",
    "        print(\"Success! Loaded with xlrd.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Standard load failed: {e}\")\n",
    "        print(\"\\n--- ATTEMPT 2: Checking if file is actually HTML/XML (Web Export) ---\")\n",
    "        # Sometimes websites export HTML tables named as .xls\n",
    "        try:\n",
    "            dfs = pd.read_html(file_path, header=[0, 1])\n",
    "            if dfs:\n",
    "                df_crop_sample = dfs[0]\n",
    "                print(\"Success! File was actually an HTML table.\")\n",
    "                print(df_crop_sample.head())\n",
    "        except Exception as html_e:\n",
    "            print(f\"HTML load also failed: {html_e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"File not found at: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70d1d73f1f286cf",
   "metadata": {},
   "source": [
    "# Step 7: ETL Pipeline — Cleaning and Reshaping Crop Data\n",
    "\n",
    "**Objective:**\n",
    "Transform wide-format crop data into a normalized long-format dataset.\n",
    "\n",
    "**Transformation Steps:**\n",
    "\n",
    "### 1. Header Parsing\n",
    "Extract metric names (`Area`, `Production`, `Yield`) embedded within the first data row.\n",
    "\n",
    "### 2. Identifier Cleaning\n",
    "- Remove numbering prefixes from `State` and `District`.\n",
    "- Convert year ranges into integer years.\n",
    "\n",
    "### 3. Reshaping (Wide to Long)\n",
    "Convert crop-specific columns into rows so each record represents:\n",
    "\n",
    "- `State`\n",
    "- `District`\n",
    "- `Year`\n",
    "- `Crop`\n",
    "- `Area`\n",
    "- `Production`\n",
    "- `Yield`\n",
    "\n",
    "**Output:**\n",
    "`Final_Crop_Data_2010_2022.xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2d32febe4cfdabd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T16:48:35.509952Z",
     "start_time": "2026-02-08T16:48:22.509540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. INITIAL PROCESSING ---\n",
      "Sample Metrics found: <StringArray>\n",
      "['Area (Hectare)', 'Production (Tonnes)', 'Yield (Tonne/Hectare)']\n",
      "Length: 3, dtype: str\n",
      "\n",
      "--- 2. CLEANING IDENTIFIERS ---\n",
      "\n",
      "--- 3. RESHAPING (UNPIVOTING) ---\n",
      "\n",
      "SUCCESS! Final Dataset Shape: (29578, 7)\n",
      "Saved to: Crop_results/Final_Crop_Data_2010_2022.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "file_path = r\"Crop.xls\"\n",
    "output_folder = r\"Crop_results\"\n",
    "\n",
    "# ✅ NEW: Create output directory if it does not exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 1. Load the HTML table\n",
    "dfs = pd.read_html(file_path, header=[0, 1])\n",
    "df_raw = dfs[0]\n",
    "\n",
    "print(\"--- 1. INITIAL PROCESSING ---\")\n",
    "\n",
    "# 2. Extract Metric Names from the first row of actual data\n",
    "metrics_row = df_raw.iloc[0].values\n",
    "print(f\"Sample Metrics found: {metrics_row[3:6]}\")\n",
    "\n",
    "# 3. Construct New Column Names\n",
    "new_columns = ['State', 'District', 'Year']\n",
    "raw_cols = df_raw.columns.tolist()\n",
    "\n",
    "for i in range(3, len(raw_cols)):\n",
    "    crop_name = raw_cols[i][0]\n",
    "    metric_raw = str(metrics_row[i])\n",
    "    if 'Area' in metric_raw:\n",
    "        metric = 'Area'\n",
    "    elif 'Production' in metric_raw:\n",
    "        metric = 'Production'\n",
    "    elif 'Yield' in metric_raw:\n",
    "        metric = 'Yield'\n",
    "    else:\n",
    "        metric = 'Unknown'\n",
    "    new_columns.append(f\"{crop_name}_{metric}\")\n",
    "\n",
    "df_raw.columns = new_columns\n",
    "df_raw = df_raw.drop(0).reset_index(drop=True)\n",
    "\n",
    "# 4. Clean State, District, and Year\n",
    "print(\"\\n--- 2. CLEANING IDENTIFIERS ---\")\n",
    "\n",
    "def clean_name(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return re.sub(r'^\\d+\\.\\s*', '', str(text)).strip()\n",
    "\n",
    "def clean_year(text):\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    try:\n",
    "        return int(str(text).split('-')[0].strip())\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df_raw['State'] = df_raw['State'].apply(clean_name)\n",
    "df_raw['District'] = df_raw['District'].apply(clean_name)\n",
    "df_raw['Year'] = df_raw['Year'].apply(clean_year)\n",
    "\n",
    "# 5. RESHAPE: Wide to Long\n",
    "print(\"\\n--- 3. RESHAPING (UNPIVOTING) ---\")\n",
    "\n",
    "processed_frames = []\n",
    "crop_cols = [c for c in df_raw.columns if '_' in c]\n",
    "unique_crops = {c.split('_')[0] for c in crop_cols}\n",
    "\n",
    "for crop in unique_crops:\n",
    "    cols_to_keep = [\n",
    "        'State', 'District', 'Year',\n",
    "        f\"{crop}_Area\",\n",
    "        f\"{crop}_Production\",\n",
    "        f\"{crop}_Yield\"\n",
    "    ]\n",
    "\n",
    "    temp_df = df_raw[cols_to_keep].copy()\n",
    "    temp_df.columns = ['State', 'District', 'Year', 'Area', 'Production', 'Yield']\n",
    "    temp_df['Crop'] = crop\n",
    "    processed_frames.append(temp_df)\n",
    "\n",
    "df_final_crop = pd.concat(processed_frames, ignore_index=True)\n",
    "\n",
    "# 6. Final Clean: Numeric Conversions & Handling NaNs\n",
    "cols_numeric = ['Area', 'Production', 'Yield']\n",
    "for col in cols_numeric:\n",
    "    df_final_crop[col] = pd.to_numeric(df_final_crop[col], errors='coerce')\n",
    "\n",
    "df_final_crop = df_final_crop.dropna(subset=cols_numeric, how='all')\n",
    "df_final_crop[cols_numeric] = df_final_crop[cols_numeric].fillna(0)\n",
    "\n",
    "print(f\"\\nSUCCESS! Final Dataset Shape: {df_final_crop.shape}\")\n",
    "\n",
    "# 7. Save\n",
    "output_file = os.path.join(output_folder, \"Final_Crop_Data_2010_2022.xlsx\")\n",
    "df_final_crop.to_excel(output_file, index=False)\n",
    "print(f\"Saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4fe7131f48d858",
   "metadata": {},
   "source": [
    "# Part 3: Data Integration and Merging\n",
    "\n",
    "# Step 8: Gap Analysis and Manual Mapping\n",
    "\n",
    "**Objective:**\n",
    "Identify district mismatches between rainfall and crop datasets.\n",
    "\n",
    "**Examples of Issues:**\n",
    "- \"SPSR Nellore\" vs \"Nellore\"\n",
    "- Minor spelling variations\n",
    "- Case sensitivity differences\n",
    "\n",
    "**Process:**\n",
    "\n",
    "1. Convert names to uppercase for uniform comparison.\n",
    "2. Compare crop districts against valid rainfall districts.\n",
    "3. Generate:\n",
    "   - `reference_district_list.txt`\n",
    "   - `manual_mapping_worksheet.csv`\n",
    "\n",
    "These files allow manual correction of mismatched district names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfe9939b6b1161f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T16:48:46.327018Z",
     "start_time": "2026-02-08T16:48:40.133472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GENERATING MANUAL MAPPING WORKSHEET ---\n",
      "Worksheet created at: rain_fall\\results\\manual_mapping_worksheet.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import difflib\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "crop_file = r\"Crop_results/Final_Crop_Data_2010_2022.xlsx\"\n",
    "db_path = r\"rain_fall\\results\\Weather_OLTP.db\"\n",
    "output_csv = r\"rain_fall\\results\\manual_mapping_worksheet.csv\"\n",
    "reference_txt = r\"rain_fall\\results\\reference_district_list.txt\"\n",
    "\n",
    "print(\"--- GENERATING MANUAL MAPPING WORKSHEET ---\")\n",
    "\n",
    "# 1. Load Data\n",
    "df_crop = pd.read_excel(crop_file)\n",
    "conn = sqlite3.connect(db_path)\n",
    "df_db_districts = pd.read_sql(\"SELECT * FROM Districts\", conn)\n",
    "df_db_states = pd.read_sql(\"SELECT * FROM States\", conn)\n",
    "conn.close()\n",
    "\n",
    "# 2. Prepare Lists\n",
    "df_crop['State_Upper'] = df_crop['State'].str.upper().str.strip()\n",
    "df_crop['District_Upper'] = df_crop['District'].str.upper().str.strip()\n",
    "\n",
    "df_valid = df_db_districts.merge(df_db_states, on='StateID')\n",
    "df_valid['StateName_Upper'] = df_valid['StateName'].str.upper().str.strip()\n",
    "df_valid['DistrictName_Upper'] = df_valid['DistrictName'].str.upper().str.strip()\n",
    "\n",
    "# 3. Create Reference List\n",
    "with open(reference_txt, \"w\") as f:\n",
    "    f.write(\"=== REFERENCE LIST ===\\n\")\n",
    "    for state in sorted(df_valid['StateName_Upper'].unique()):\n",
    "        f.write(f\"--- {state} ---\\n\")\n",
    "        dists = sorted(df_valid[df_valid['StateName_Upper'] == state]['DistrictName_Upper'].tolist())\n",
    "        for d in dists: f.write(f\"{d}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# 4. Identify Mismatches\n",
    "unique_crop_locs = df_crop[['State_Upper', 'District_Upper']].drop_duplicates()\n",
    "mapping_data = []\n",
    "\n",
    "for index, row in unique_crop_locs.iterrows():\n",
    "    state = row['State_Upper']\n",
    "    dist = row['District_Upper']\n",
    "    valid_options = df_valid[df_valid['StateName_Upper'] == state]['DistrictName_Upper'].tolist()\n",
    "\n",
    "    if dist in valid_options: continue\n",
    "\n",
    "    matches = difflib.get_close_matches(dist, valid_options, n=1, cutoff=0.0)\n",
    "    suggestion = matches[0] if matches else \"\"\n",
    "\n",
    "    mapping_data.append({\n",
    "        'State': state,\n",
    "        'Crop_District_Original': dist,\n",
    "        'Auto_Suggestion': suggestion,\n",
    "        'CORRECT_NAME_FROM_DB': suggestion\n",
    "    })\n",
    "\n",
    "# 5. Save the Worksheet\n",
    "df_map = pd.DataFrame(mapping_data).sort_values(by=['State', 'Crop_District_Original'])\n",
    "df_map.to_csv(output_csv, index=False)\n",
    "print(f\"Worksheet created at: {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddcc4a5bc5a89b1",
   "metadata": {},
   "source": [
    "# Part 4: Final Integration & Engineering\n",
    "\n",
    "# Step 9: Applying Manual Geographic Mapping\n",
    "\n",
    "**Objective:**\n",
    "Resolve mismatched District names between the Agriculture and Climate datasets.\n",
    "\n",
    "**Methodology:**\n",
    "\n",
    "Instead of relying on fuzzy matching (which produced low accuracy), we apply a hardcoded dictionary containing 333 manual corrections.\n",
    "\n",
    "- **Source:** Manual audit of the Mismatch Report\n",
    "- **Logic:** Maps district name variations such as\n",
    "  `VISAKHAPATANAM` → `VISAKHAPATNAM`\n",
    "- **Execution:** A new column `District_Final` is created and used as the joining key.\n",
    "\n",
    "**Output:**\n",
    "`Final_Merged_Dataset_Clean.xlsx` (Preliminary Merge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13a8321933ce4043",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T16:54:43.570492Z",
     "start_time": "2026-02-08T16:54:25.662775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 333 manual mapping rules.\n",
      "\n",
      "Loading Crop Data...\n",
      "Loading Rainfall Data...\n",
      "\n",
      "Merging Datasets...\n",
      "------------------------------\n",
      "Total Crop Records: 29578\n",
      "Records with Rainfall Data: 20298\n",
      "Final Match Rate: 68.6%\n",
      "------------------------------\n",
      "\n",
      "SUCCESS! Master dataset saved to:\n",
      "rain_fall/results/Final_Merged_Dataset_Clean.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "\n",
    "# --- 1. SETUP PATHS ---\n",
    "folder_path = r\"rain_fall/results\"\n",
    "crop_file = os.path.join(folder_path, \"Final_Crop_Data_2010_2022.xlsx\")\n",
    "rain_file = os.path.join(folder_path, \"Final_Rainfall_Data_2010_2022.xlsx\")\n",
    "output_file = os.path.join(folder_path, \"Final_Merged_Dataset_Clean.xlsx\")\n",
    "\n",
    "# --- 2. MANUAL MAPPING DATA (Embedded) ---\n",
    "csv_content = \"\"\"State,Crop_District_Original,Auto_Suggestion,CORRECT_NAME_FROM_DB\n",
    "ANDAMAN AND NICOBAR ISLANDS,NICOBARS,NICOBAR,NICOBAR\n",
    "ANDAMAN AND NICOBAR ISLANDS,NORTH AND MIDDLE ANDAMAN,NORTH & MIDDLE ANDAMAN,NORTH & MIDDLE ANDAMAN\n",
    "ANDAMAN AND NICOBAR ISLANDS,SOUTH ANDAMANS,SOUTH ANDAMAN,SOUTH ANDAMAN\n",
    "ANDHRA PRADESH,ANANTAPUR,ANATAPUR,ANATAPUR\n",
    "ANDHRA PRADESH,KADAPA,KADDAPA,KADDAPA\n",
    "ANDHRA PRADESH,SPSR NELLORE,NELLORE,NELLORE\n",
    "ANDHRA PRADESH,VISAKHAPATANAM,VISAKHAPATNAM,VISAKHAPATNAM\n",
    "ANDHRA PRADESH,VIZIANAGARAM,ANATAPUR,ANATAPUR\n",
    "ARUNACHAL PRADESH,ANJAW,PAPUM PARE,PAPUM PARE\n",
    "ARUNACHAL PRADESH,CHANGLANG,EAST SIANG,EAST SIANG\n",
    "ARUNACHAL PRADESH,DIBANG VALLEY,PAPUM PARE,PAPUM PARE\n",
    "ARUNACHAL PRADESH,EAST KAMENG,EAST SIANG,EAST SIANG\n",
    "ARUNACHAL PRADESH,KAMLE,PAPUM PARE,PAPUM PARE\n",
    "ARUNACHAL PRADESH,KRA DAADI,PAPUM PARE,PAPUM PARE\n",
    "ARUNACHAL PRADESH,KURUNG KUMEY,PAPUM PARE,PAPUM PARE\n",
    "ARUNACHAL PRADESH,LEPARADA,PAPUM PARE,PAPUM PARE\n",
    "ARUNACHAL PRADESH,LOHIT,TIRAP,TIRAP\n",
    "ARUNACHAL PRADESH,LONGDING,EAST SIANG,EAST SIANG\n",
    "ARUNACHAL PRADESH,LOWER DIBANG VALLEY,EAST SIANG,EAST SIANG\n",
    "ARUNACHAL PRADESH,LOWER SIANG,EAST SIANG,EAST SIANG\n",
    "ARUNACHAL PRADESH,LOWER SUBANSIRI,EAST SIANG,EAST SIANG\n",
    "ARUNACHAL PRADESH,NAMSAI,PAPUM PARE,PAPUM PARE\n",
    "ARUNACHAL PRADESH,PAKKE KESSANG,EAST SIANG,EAST SIANG\n",
    "ARUNACHAL PRADESH,SHI YOMI,EAST SIANG,EAST SIANG\n",
    "ARUNACHAL PRADESH,SIANG,EAST SIANG,EAST SIANG\n",
    "ARUNACHAL PRADESH,TAWANG,EAST SIANG,EAST SIANG\n",
    "ARUNACHAL PRADESH,UPPER SIANG,EAST SIANG,EAST SIANG\n",
    "ARUNACHAL PRADESH,UPPER SUBANSIRI,EAST SIANG,EAST SIANG\n",
    "ARUNACHAL PRADESH,WEST KAMENG,EAST SIANG,EAST SIANG\n",
    "ARUNACHAL PRADESH,WEST SIANG,EAST SIANG,EAST SIANG\n",
    "ASSAM,BAKSA,DIMA HASAO,DIMA HASAO\n",
    "ASSAM,BARPETA,KAMRUP (METRO),KAMRUP (METRO)\n",
    "ASSAM,BISWANATH,SONITPUR,SONITPUR\n",
    "ASSAM,BONGAIGAON,NOWGAON,NOWGAON\n",
    "ASSAM,CHARAIDEO,CACHAR,CACHAR\n",
    "ASSAM,CHIRANG,DARRANG,DARRANG\n",
    "ASSAM,DHEMAJI,DHUBRI,DHUBRI\n",
    "ASSAM,HAILAKANDI,GOALPARA,GOALPARA\n",
    "ASSAM,HOJAI,JORHAT,JORHAT\n",
    "ASSAM,KAMRUP,KAMRUP (RURAL),KAMRUP (RURAL)\n",
    "ASSAM,KAMRUP METRO,KAMRUP (METRO),KAMRUP (METRO)\n",
    "ASSAM,KARBI ANGLONG,DARRANG,DARRANG\n",
    "ASSAM,KARIMGANJ,DARRANG,DARRANG\n",
    "ASSAM,KOKRAJHAR,CACHAR,CACHAR\n",
    "ASSAM,MAJULI,UDALGURI,UDALGURI\n",
    "ASSAM,MARIGAON,NOWGAON,NOWGAON\n",
    "ASSAM,NAGAON,NOWGAON,NOWGAON\n",
    "ASSAM,NALBARI,UDALGURI,UDALGURI\n",
    "ASSAM,SIVASAGAR,DIBRUGARH,DIBRUGARH\n",
    "ASSAM,SOUTH SALMARA MANCACHAR,CACHAR,CACHAR\n",
    "ASSAM,TINSUKIA,DIBRUGARH,DIBRUGARH\n",
    "ASSAM,WEST KARBI ANGLONG,DARRANG,DARRANG\n",
    "BIHAR,ARWAL,SARAN,SARAN\n",
    "BIHAR,AURANGABAD,DARBHANGA,DARBHANGA\n",
    "BIHAR,BANKA,PATNA,PATNA\n",
    "BIHAR,BEGUSARAI,SARAN,SARAN\n",
    "BIHAR,BHOJPUR,BHAGALPUR,BHAGALPUR\n",
    "BIHAR,BUXAR,BHAGALPUR,BHAGALPUR\n",
    "BIHAR,GOPALGANJ,SARAN,SARAN\n",
    "BIHAR,JAMUI,ARARIA,ARARIA\n",
    "BIHAR,JEHANABAD,DARBHANGA,DARBHANGA\n",
    "BIHAR,KAIMUR (BHABUA),DARBHANGA,DARBHANGA\n",
    "BIHAR,KATIHAR,PATNA,PATNA\n",
    "BIHAR,KHAGARIA,ARARIA,ARARIA\n",
    "BIHAR,KISHANGANJ,DARBHANGA,DARBHANGA\n",
    "BIHAR,LAKHISARAI,SARAN,SARAN\n",
    "BIHAR,MADHEPURA,PURNEA,PURNEA\n",
    "BIHAR,MADHUBANI,DARBHANGA,DARBHANGA\n",
    "BIHAR,MUNGER,MUZAFFARPUR,MUZAFFARPUR\n",
    "BIHAR,NALANDA,SARAN,SARAN\n",
    "BIHAR,NAWADA,ARARIA,ARARIA\n",
    "BIHAR,PASHCHIM CHAMPARAN,SARAN,SARAN\n",
    "BIHAR,PURBI CHAMPARAN,SARAN,SARAN\n",
    "BIHAR,PURNIA,PURNEA,PURNEA\n",
    "BIHAR,SAHARSA,SARAN,SARAN\n",
    "BIHAR,SAMASTIPUR,BHAGALPUR,BHAGALPUR\n",
    "BIHAR,SHEIKHPURA,PURNEA,PURNEA\n",
    "BIHAR,SHEOHAR,SARAN,SARAN\n",
    "BIHAR,SITAMARHI,SARAN,SARAN\n",
    "BIHAR,SIWAN,SARAN,SARAN\n",
    "BIHAR,VAISHALI,SUPAUL,SUPAUL\n",
    "CHANDIGARH,CHANDIGARH,,\n",
    "CHHATTISGARH,BALOD,BASTAR,BASTAR\n",
    "CHHATTISGARH,BALODA BAZAR,BASTAR,BASTAR\n",
    "CHHATTISGARH,BALRAMPUR,BILASPUR,BILASPUR\n",
    "CHHATTISGARH,BEMETARA,BASTAR,BASTAR\n",
    "CHHATTISGARH,BIJAPUR,BILASPUR,BILASPUR\n",
    "CHHATTISGARH,DANTEWADA,BASTAR,BASTAR\n",
    "CHHATTISGARH,DHAMTARI,BASTAR,BASTAR\n",
    "CHHATTISGARH,GARIYABAND,SARGUJA,SARGUJA\n",
    "CHHATTISGARH,GAURELLA-PENDRA-MARWAHI,BILASPUR,BILASPUR\n",
    "CHHATTISGARH,JANJGIR-CHAMPA,JANJGIR,JANJGIR\n",
    "CHHATTISGARH,JASHPUR,BILASPUR,BILASPUR\n",
    "CHHATTISGARH,KABIRDHAM,SARGUJA,SARGUJA\n",
    "CHHATTISGARH,KANKER,JANJGIR,JANJGIR\n",
    "CHHATTISGARH,KONDAGAON,SARGUJA,SARGUJA\n",
    "CHHATTISGARH,KORBA,RAIPUR,RAIPUR\n",
    "CHHATTISGARH,KOREA,RAIPUR,RAIPUR\n",
    "CHHATTISGARH,MAHASAMUND,BASTAR,BASTAR\n",
    "CHHATTISGARH,MUNGELI,JANJGIR,JANJGIR\n",
    "CHHATTISGARH,NARAYANPUR,RAIPUR,RAIPUR\n",
    "CHHATTISGARH,RAIGARH,RAIPUR,RAIPUR\n",
    "CHHATTISGARH,RAJNANDGAON,JANJGIR,JANJGIR\n",
    "CHHATTISGARH,SUKMA,SARGUJA,SARGUJA\n",
    "CHHATTISGARH,SURAJPUR,RAIPUR,RAIPUR\n",
    "CHHATTISGARH,SURGUJA,SARGUJA,SARGUJA\n",
    "DELHI,DELHI_TOTAL,SOUTH DELHI,SOUTH DELHI\n",
    "GOA,NORTH GOA,SOUTH GOA,SOUTH GOA\n",
    "GUJARAT,AHMADABAD,AHMEDABAD,AHMEDABAD\n",
    "GUJARAT,ANAND,JUNAGAD,JUNAGAD\n",
    "GUJARAT,ARAVALLI,AMRELI,AMRELI\n",
    "GUJARAT,BANAS KANTHA,BANASKANTHA,BANASKANTHA\n",
    "GUJARAT,BHARUCH,KUTCH,KUTCH\n",
    "GUJARAT,BOTAD,AHMEDABAD,AHMEDABAD\n",
    "GUJARAT,CHHOTAUDEPUR,VADODRA,VADODRA\n",
    "GUJARAT,DANG,JAMNAGAR,JAMNAGAR\n",
    "GUJARAT,DEVBHUMI DWARKA,SABARKANTHA,SABARKANTHA\n",
    "GUJARAT,DOHAD,AHMEDABAD,AHMEDABAD\n",
    "GUJARAT,GIR SOMNATH,SURAT,SURAT\n",
    "GUJARAT,KACHCHH,KUTCH,KUTCH\n",
    "GUJARAT,KHEDA,KAIRA KHEDA,KAIRA KHEDA\n",
    "GUJARAT,MAHESANA,AHMEDABAD,AHMEDABAD\n",
    "GUJARAT,MAHISAGAR,GANDHINAGAR,GANDHINAGAR\n",
    "GUJARAT,MORBI,AMRELI,AMRELI\n",
    "GUJARAT,NARMADA,JUNAGAD,JUNAGAD\n",
    "GUJARAT,NAVSARI,VALSAR,VALSAR\n",
    "GUJARAT,PANCH MAHALS,GANDHINAGAR,GANDHINAGAR\n",
    "GUJARAT,PATAN,SURAT,SURAT\n",
    "GUJARAT,PORBANDAR,BHAVNAGAR,BHAVNAGAR\n",
    "GUJARAT,SABAR KANTHA,SABARKANTHA,SABARKANTHA\n",
    "GUJARAT,TAPI,AMRELI,AMRELI\n",
    "GUJARAT,VADODARA,VADODRA,VADODRA\n",
    "GUJARAT,VALSAD,VALSAR,VALSAR\n",
    "HARYANA,CHARKI DADRI,CHANDIGARH,CHANDIGARH\n",
    "HARYANA,FARIDABAD,KARNAL,KARNAL\n",
    "HARYANA,FATEHABAD,AMBALA,AMBALA\n",
    "HARYANA,GURGAON,ROHTAK,ROHTAK\n",
    "HARYANA,HISAR,HISSAR,HISSAR\n",
    "HARYANA,JHAJJAR,CHANDIGARH,CHANDIGARH\n",
    "HARYANA,JIND,BHIWANI,BHIWANI\n",
    "HARYANA,KAITHAL,KARNAL,KARNAL\n",
    "HARYANA,KURUKSHETRA,ROHTAK,ROHTAK\n",
    "HARYANA,MAHENDRAGARH,CHANDIGARH,CHANDIGARH\n",
    "HARYANA,MEWAT,BHIWANI,BHIWANI\n",
    "HARYANA,PALWAL,AMBALA,AMBALA\n",
    "HARYANA,PANCHKULA,AMBALA,AMBALA\n",
    "HARYANA,PANIPAT,CHANDIGARH,CHANDIGARH\n",
    "HARYANA,REWARI,BHIWANI,BHIWANI\n",
    "HARYANA,SIRSA,HISSAR,HISSAR\n",
    "HARYANA,SONIPAT,ROHTAK,ROHTAK\n",
    "HARYANA,YAMUNANAGAR,CHANDIGARH,CHANDIGARH\n",
    "HIMACHAL PRADESH,BILASPUR,KINNAUR,KINNAUR\n",
    "HIMACHAL PRADESH,HAMIRPUR,KINNAUR,KINNAUR\n",
    "HIMACHAL PRADESH,SIRMAUR,SHIMLA,SHIMLA\n",
    "JAMMU AND KASHMIR,KISHTWAR,KUPWARA,KUPWARA\n",
    "JAMMU AND KASHMIR,SAMBA,RAMBAN,RAMBAN\n",
    "JHARKHAND,CHATRA,RANCHI,RANCHI\n",
    "JHARKHAND,EAST SINGHBUM,EAST SINGHBHUM,EAST SINGHBHUM\n",
    "JHARKHAND,GARHWA,RANCHI,RANCHI\n",
    "JHARKHAND,GODDA,RANCHI,RANCHI\n",
    "JHARKHAND,GUMLA,PALAMAU,PALAMAU\n",
    "JHARKHAND,HAZARIBAGH,RANCHI,RANCHI\n",
    "JHARKHAND,KODERMA,RANCHI,RANCHI\n",
    "JHARKHAND,LATEHAR,PALAMAU,PALAMAU\n",
    "JHARKHAND,LOHARDAGA,PALAMAU,PALAMAU\n",
    "JHARKHAND,PALAMU,PALAMAU,PALAMAU\n",
    "JHARKHAND,SAHEBGANJ,RANCHI,RANCHI\n",
    "JHARKHAND,SARAIKELA KHARSAWAN,PALAMAU,PALAMAU\n",
    "KARNATAKA,BANGALORE RURAL,BENGALURU,BENGALURU\n",
    "KARNATAKA,CHAMARAJANAGAR,CHAMARAJNAGAR,CHAMARAJNAGAR\n",
    "KARNATAKA,CHIKBALLAPUR,CHIKMAGALUR,CHIKMAGALUR\n",
    "KARNATAKA,DAKSHIN KANNAD,DAKSHIN KANNADA,DAKSHIN KANNADA\n",
    "KARNATAKA,DAVANGERE,DAVANAGERE,DAVANAGERE\n",
    "KARNATAKA,RAMANAGARA,CHAMARAJNAGAR,CHAMARAJNAGAR\n",
    "KARNATAKA,UDUPI,TUMKUR,TUMKUR\n",
    "KARNATAKA,UTTAR KANNAD,UTTAR KANNADA,UTTAR KANNADA\n",
    "KARNATAKA,YADGIR,GADAG,GADAG\n",
    "KERALA,IDUKKI,THRISSUR,THRISSUR\n",
    "KERALA,KASARAGOD,MALAPPURAM,MALAPPURAM\n",
    "KERALA,PATHANAMTHITTA,THIRUVANATHA PURAM,THIRUVANATHA PURAM\n",
    "KERALA,THIRUVANANTHAPURAM,THIRUVANATHA PURAM,THIRUVANATHA PURAM\n",
    "KERALA,WAYANAD,PALAKKAD,PALAKKAD\n",
    "MADHYA PRADESH,AGAR MALWA,SAGAR,SAGAR\n",
    "MADHYA PRADESH,ANUPPUR,SHAJAPUR,SHAJAPUR\n",
    "MADHYA PRADESH,ASHOKNAGAR,SAGAR,SAGAR\n",
    "MADHYA PRADESH,BARWANI,REWA,REWA\n",
    "MADHYA PRADESH,BHIND,INDORE,INDORE\n",
    "MADHYA PRADESH,BURHANPUR,SHAJAPUR,SHAJAPUR\n",
    "MADHYA PRADESH,DEWAS,REWA,REWA\n",
    "MADHYA PRADESH,DINDORI,INDORE,INDORE\n",
    "MADHYA PRADESH,HARDA,DHAR,DHAR\n",
    "MADHYA PRADESH,JABALPUR,JABALPUR(A),JABALPUR(A)\n",
    "MADHYA PRADESH,JHABUA,JABALPUR(A),JABALPUR(A)\n",
    "MADHYA PRADESH,KATNI,SATNA,SATNA\n",
    "MADHYA PRADESH,KHANDWA,MANDLA,MANDLA\n",
    "MADHYA PRADESH,KHARGONE,DHAR,DHAR\n",
    "MADHYA PRADESH,MANDSAUR,MANDLA,MANDLA\n",
    "MADHYA PRADESH,MORENA,REWA,REWA\n",
    "MADHYA PRADESH,NEEMUCH,DAMOH,DAMOH\n",
    "MADHYA PRADESH,NIWARI,WEST NIMAR,WEST NIMAR\n",
    "MADHYA PRADESH,PANNA,SATNA,SATNA\n",
    "MADHYA PRADESH,RATLAM,SATNA,SATNA\n",
    "MADHYA PRADESH,SEHORE,SEONI,SEONI\n",
    "MADHYA PRADESH,SHAHDOL,MANDLA,MANDLA\n",
    "MADHYA PRADESH,SINGRAULI,NARSINGHPUR,NARSINGHPUR\n",
    "MADHYA PRADESH,VIDISHA,DHAR,DHAR\n",
    "MAHARASHTRA,AHILYANAGAR,AHMEDNAGAR,AHMEDNAGAR\n",
    "MAHARASHTRA,BEED,NADED,NADED\n",
    "MAHARASHTRA,BHANDARA,CHANDRAPUR,CHANDRAPUR\n",
    "MAHARASHTRA,BULDHANA,BULDANA,BULDANA\n",
    "MAHARASHTRA,CHHATRAPATI SAMBHAJINAGAR,RATNAGIRI,RATNAGIRI\n",
    "MAHARASHTRA,DHARASHIV,WASHIM,WASHIM\n",
    "MAHARASHTRA,DHULE,PUNE,PUNE\n",
    "MAHARASHTRA,GADCHIROLI,RATNAGIRI,RATNAGIRI\n",
    "MAHARASHTRA,HINGOLI,SANGLI,SANGLI\n",
    "MAHARASHTRA,JALNA,JALGAON,JALGAON\n",
    "MAHARASHTRA,NANDED,NADED,NADED\n",
    "MAHARASHTRA,NANDURBAR,NAGPUR,NAGPUR\n",
    "MANIPUR,BISHNUPUR,IMPHAL EAST,IMPHAL EAST\n",
    "MANIPUR,CHANDEL,IMPHAL EAST,IMPHAL EAST\n",
    "MANIPUR,CHURACHANDPUR,IMPHAL EAST,IMPHAL EAST\n",
    "MANIPUR,IMPHAL WEST,IMPHAL EAST,IMPHAL EAST\n",
    "MANIPUR,SENAPATI,IMPHAL EAST,IMPHAL EAST\n",
    "MANIPUR,TAMENGLONG,IMPHAL EAST,IMPHAL EAST\n",
    "MANIPUR,THOUBAL,IMPHAL EAST,IMPHAL EAST\n",
    "MANIPUR,UKHRUL,IMPHAL EAST,IMPHAL EAST\n",
    "MEGHALAYA,EAST GARO HILLS,EAST KHASI HILLS,EAST KHASI HILLS\n",
    "MEGHALAYA,EAST JAINTIA HILLS,EAST KHASI HILLS,EAST KHASI HILLS\n",
    "MEGHALAYA,NORTH GARO HILLS,EAST KHASI HILLS,EAST KHASI HILLS\n",
    "MEGHALAYA,SOUTH GARO HILLS,EAST KHASI HILLS,EAST KHASI HILLS\n",
    "MEGHALAYA,SOUTH WEST GARO HILLS,EAST KHASI HILLS,EAST KHASI HILLS\n",
    "MEGHALAYA,SOUTH WEST KHASI HILLS,EAST KHASI HILLS,EAST KHASI HILLS\n",
    "MEGHALAYA,WEST GARO HILLS,EAST KHASI HILLS,EAST KHASI HILLS\n",
    "MEGHALAYA,WEST JAINTIA HILLS,EAST KHASI HILLS,EAST KHASI HILLS\n",
    "MEGHALAYA,WEST KHASI HILLS,EAST KHASI HILLS,EAST KHASI HILLS\n",
    "MIZORAM,AIZAWL,AIZWAL,AIZWAL\n",
    "MIZORAM,CHAMPHAI,AIZWAL,AIZWAL\n",
    "MIZORAM,HNAHTHIAL,AIZWAL,AIZWAL\n",
    "MIZORAM,KHAWZAWL,AIZWAL,AIZWAL\n",
    "MIZORAM,KOLASIB,AIZWAL,AIZWAL\n",
    "MIZORAM,LAWNGTLAI,AIZWAL,AIZWAL\n",
    "MIZORAM,LUNGLEI,AIZWAL,AIZWAL\n",
    "MIZORAM,MAMIT,AIZWAL,AIZWAL\n",
    "MIZORAM,SAIHA,AIZWAL,AIZWAL\n",
    "MIZORAM,SAITUAL,AIZWAL,AIZWAL\n",
    "MIZORAM,SERCHHIP,AIZWAL,AIZWAL\n",
    "PUDUCHERRY,KARAIKAL,PUDUCHERRY,PUDUCHERRY\n",
    "PUDUCHERRY,MAHE,PUDUCHERRY,PUDUCHERRY\n",
    "PUDUCHERRY,PONDICHERRY,PUDUCHERRY,PUDUCHERRY\n",
    "PUDUCHERRY,YANAM,PUDUCHERRY,PUDUCHERRY\n",
    "PUNJAB,BARNALA,PATIALA,PATIALA\n",
    "PUNJAB,BATHINDA,PATIALA,PATIALA\n",
    "PUNJAB,FATEHGARH SAHIB,AMRITSAR,AMRITSAR\n",
    "PUNJAB,FAZILKA,PATIALA,PATIALA\n",
    "PUNJAB,FIROZEPUR,HOSHIARPUR,HOSHIARPUR\n",
    "PUNJAB,GURDASPUR,HOSHIARPUR,HOSHIARPUR\n",
    "PUNJAB,JALANDHAR,LUDHIANA,LUDHIANA\n",
    "PUNJAB,KAPURTHALA,PATIALA,PATIALA\n",
    "PUNJAB,MANSA,LUDHIANA,LUDHIANA\n",
    "PUNJAB,MUKTSAR,AMRITSAR,AMRITSAR\n",
    "PUNJAB,NAWANSHAHR,HOSHIARPUR,HOSHIARPUR\n",
    "PUNJAB,PATHANKOT,PATIALA,PATIALA\n",
    "PUNJAB,RUPNAGAR,PATIALA,PATIALA\n",
    "PUNJAB,S.A.S NAGAR,AMRITSAR,AMRITSAR\n",
    "PUNJAB,SANGRUR,HOSHIARPUR,HOSHIARPUR\n",
    "PUNJAB,TARN TARAN,PATIALA,PATIALA\n",
    "RAJASTHAN,ALWAR,JHALAWAR,JHALAWAR\n",
    "RAJASTHAN,BARAN,BANSWARA,BANSWARA\n",
    "RAJASTHAN,BHARATPUR,JAIPUR,JAIPUR\n",
    "RAJASTHAN,BHILWARA,JHALAWAR,JHALAWAR\n",
    "RAJASTHAN,DAUSA,UDAIPUR,UDAIPUR\n",
    "RAJASTHAN,DUNGARPUR,UDAIPUR,UDAIPUR\n",
    "RAJASTHAN,GANGANAGAR,SRIGANGANAGAR,SRIGANGANAGAR\n",
    "RAJASTHAN,HANUMANGARH,JHALAWAR,JHALAWAR\n",
    "RAJASTHAN,JHUNJHUNU,CHURU,CHURU\n",
    "RAJASTHAN,KARAULI,PALI,PALI\n",
    "RAJASTHAN,NAGAUR,SRIGANGANAGAR,SRIGANGANAGAR\n",
    "RAJASTHAN,PRATAPGARH,SRIGANGANAGAR,SRIGANGANAGAR\n",
    "RAJASTHAN,RAJSAMAND,JAISALMER,JAISALMER\n",
    "RAJASTHAN,SIKAR,BIKANER,BIKANER\n",
    "TAMIL NADU,ARIYALUR,KARUR,KARUR\n",
    "TAMIL NADU,CHENGALPATTU,NAGAPATTINAM,NAGAPATTINAM\n",
    "TAMIL NADU,ERODE,VELLORE,VELLORE\n",
    "TAMIL NADU,KALLAKURICHI,KANYAKUMARI,KANYAKUMARI\n",
    "TAMIL NADU,KANCHIPURAM,RAMANATHAPURAM,RAMANATHAPURAM\n",
    "TAMIL NADU,KANNIYAKUMARI,KANYAKUMARI,KANYAKUMARI\n",
    "TAMIL NADU,KRISHNAGIRI,NILGIRIS,NILGIRIS\n",
    "TAMIL NADU,MAYILADUTHURAI,MADURAI,MADURAI\n",
    "TAMIL NADU,NAMAKKAL,PERAMBALUR,PERAMBALUR\n",
    "TAMIL NADU,PUDUKKOTTAI,MADURAI,MADURAI\n",
    "TAMIL NADU,RANIPET,MADURAI,MADURAI\n",
    "TAMIL NADU,SIVAGANGA,VIRUDHUNAGAR,VIRUDHUNAGAR\n",
    "TAMIL NADU,THE NILGIRIS,NILGIRIS,NILGIRIS\n",
    "TAMIL NADU,THENI,CHENNAI,CHENNAI\n",
    "TAMIL NADU,THENKASI,CHENNAI,CHENNAI\n",
    "TAMIL NADU,THIRUVALLUR,TIRUVALLUR,TIRUVALLUR\n",
    "TAMIL NADU,THIRUVARUR,TIRUVALLUR,TIRUVALLUR\n",
    "TAMIL NADU,TIRUCHIRAPPALLI,TIRUCHIRAPALLI,TIRUCHIRAPALLI\n",
    "TAMIL NADU,TIRUPATHUR,TIRUPATTUR,TIRUPATTUR\n",
    "TAMIL NADU,TIRUPPUR,TIRUPATTUR,TIRUPATTUR\n",
    "TAMIL NADU,TIRUVANNAMALAI,TIRUVALLUR,TIRUVALLUR\n",
    "TAMIL NADU,VILLUPURAM,TIRUVALLUR,TIRUVALLUR\n",
    "TELANGANA,KARIMNAGAR,MAHBOOB NAGAR,MAHBOOB NAGAR\n",
    "TELANGANA,MAHBUBNAGAR,MAHBOOB NAGAR,MAHBOOB NAGAR\n",
    "TELANGANA,RANGAREDDI,WARANGAL URBAN,WARANGAL URBAN\n",
    "TELANGANA,WARANGAL,WARANGAL URBAN,WARANGAL URBAN\n",
    "TRIPURA,DHALAI,NORTH TRIPURA,NORTH TRIPURA\n",
    "TRIPURA,GOMATI,NORTH TRIPURA,NORTH TRIPURA\n",
    "TRIPURA,KHOWAI,WEST TRIPURA,WEST TRIPURA\n",
    "TRIPURA,SEPAHIJALA,WEST TRIPURA,WEST TRIPURA\n",
    "TRIPURA,SOUTH TRIPURA,NORTH TRIPURA,NORTH TRIPURA\n",
    "TRIPURA,UNAKOTI,NORTH TRIPURA,NORTH TRIPURA\n",
    "UTTARAKHAND,ALMORA,HARIDWAR,HARIDWAR\n",
    "UTTARAKHAND,BAGESHWAR,HARIDWAR,HARIDWAR\n",
    "UTTARAKHAND,CHAMOLI,HARIDWAR,HARIDWAR\n",
    "UTTARAKHAND,CHAMPAWAT,HARIDWAR,HARIDWAR\n",
    "UTTARAKHAND,DEHRADUN,DEHRA DUN,DEHRA DUN\n",
    "UTTARAKHAND,PAURI GARHWAL,NAINITAL,NAINITAL\n",
    "UTTARAKHAND,PITHORAGARH,UDHAM SINGH NAGAR,UDHAM SINGH NAGAR\n",
    "UTTARAKHAND,RUDRA PRAYAG,UDHAM SINGH NAGAR,UDHAM SINGH NAGAR\n",
    "UTTARAKHAND,TEHRI GARHWAL,TEHRI NEW,TEHRI NEW\n",
    "UTTARAKHAND,UDAM SINGH NAGAR,UDHAM SINGH NAGAR,UDHAM SINGH NAGAR\n",
    "UTTARAKHAND,UTTAR KASHI,HARIDWAR,HARIDWAR\n",
    "WEST BENGAL,24 PARAGANAS NORTH,SOUTH 24 PARGANAS,SOUTH 24 PARGANAS\n",
    "WEST BENGAL,24 PARAGANAS SOUTH,SOUTH 24 PARGANAS,SOUTH 24 PARGANAS\n",
    "WEST BENGAL,ALIPURDUAR,PURULIA,PURULIA\n",
    "WEST BENGAL,COOCHBEHAR,COOCH BEHAR,COOCH BEHAR\n",
    "WEST BENGAL,DINAJPUR DAKSHIN,SOUTH DINAJPUR,SOUTH DINAJPUR\n",
    "WEST BENGAL,DINAJPUR UTTAR,SOUTH DINAJPUR,SOUTH DINAJPUR\n",
    "WEST BENGAL,HOOGHLY,HOOGLY,HOOGLY\n",
    "WEST BENGAL,JHARGRAM,HOWRAH,HOWRAH\n",
    "WEST BENGAL,MEDINIPUR EAST,PASCHIM MEDNAPUR,PASCHIM MEDNAPUR\n",
    "WEST BENGAL,MEDINIPUR WEST,PASCHIM MEDNAPUR,PASCHIM MEDNAPUR\n",
    "\"\"\"\n",
    "\n",
    "# Read into DataFrame\n",
    "df_map = pd.read_csv(io.StringIO(csv_content))\n",
    "print(f\"Loaded {len(df_map)} manual mapping rules.\")\n",
    "\n",
    "# --- 3. APPLY CORRECTIONS ---\n",
    "print(\"\\nLoading Crop Data...\")\n",
    "df_crop = pd.read_excel(crop_file)\n",
    "\n",
    "# Build Correction Dictionary: { 'VISAKHAPATANAM': 'VISAKHAPATNAM' }\n",
    "# We filter out rows where correct name is missing\n",
    "df_map = df_map.dropna(subset=['CORRECT_NAME_FROM_DB'])\n",
    "correction_dict = dict(zip(df_map['Crop_District_Original'], df_map['CORRECT_NAME_FROM_DB']))\n",
    "\n",
    "# Standardize Crop Data keys\n",
    "df_crop['State_Upper'] = df_crop['State'].str.upper().str.strip()\n",
    "df_crop['District_Upper'] = df_crop['District'].str.upper().str.strip()\n",
    "\n",
    "# Create 'District_Final' column\n",
    "# Logic: If in dict, use dict value. Else, use original upper case name.\n",
    "df_crop['District_Final'] = df_crop['District_Upper'].map(correction_dict).fillna(df_crop['District_Upper'])\n",
    "\n",
    "# --- 4. PREPARE RAINFALL DATA ---\n",
    "print(\"Loading Rainfall Data...\")\n",
    "df_rain = pd.read_excel(rain_file)\n",
    "\n",
    "# Aggregate Monthly -> Annual Rainfall\n",
    "# Note: We group by State/District/Year.\n",
    "rain_summary = df_rain.groupby(['State', 'District', 'Year'])['Rainfall (mm)'].sum().reset_index()\n",
    "rain_summary.rename(columns={'Rainfall (mm)': 'Annual_Rainfall'}, inplace=True)\n",
    "\n",
    "# Standardize Rainfall keys\n",
    "rain_summary['State_Upper'] = rain_summary['State'].str.upper().str.strip()\n",
    "rain_summary['District_Upper'] = rain_summary['District'].str.upper().str.strip()\n",
    "\n",
    "# --- 5. MERGE ---\n",
    "print(\"\\nMerging Datasets...\")\n",
    "\n",
    "# Left Join: Keep all Crop records, attach Rain where available\n",
    "df_merged = pd.merge(\n",
    "    df_crop,\n",
    "    rain_summary,\n",
    "    how='left',\n",
    "    left_on=['State_Upper', 'District_Final', 'Year'],\n",
    "    right_on=['State_Upper', 'District_Upper', 'Year']\n",
    ")\n",
    "\n",
    "# --- 6. CLEAN UP & SAVE ---\n",
    "# Select only useful columns\n",
    "cols_to_keep = [\n",
    "    'State_x', 'District_Final', 'Year', 'Crop',\n",
    "    'Area', 'Production', 'Yield', 'Annual_Rainfall'\n",
    "]\n",
    "\n",
    "df_final = df_merged[cols_to_keep].copy()\n",
    "df_final.rename(columns={\n",
    "    'State_x': 'State',\n",
    "    'District_Final': 'District'\n",
    "}, inplace=True)\n",
    "\n",
    "# Stats\n",
    "total_rows = len(df_final)\n",
    "matched_rows = df_final['Annual_Rainfall'].notna().sum()\n",
    "match_percentage = (matched_rows / total_rows) * 100\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Total Crop Records: {total_rows}\")\n",
    "print(f\"Records with Rainfall Data: {matched_rows}\")\n",
    "print(f\"Final Match Rate: {match_percentage:.1f}%\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Save\n",
    "df_final.to_excel(output_file, index=False)\n",
    "print(f\"\\nSUCCESS! Master dataset saved to:\\n{output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fc18811f71c46",
   "metadata": {},
   "source": [
    "# Step 10: Final Engineering (Aggregation & Validation)\n",
    "\n",
    "**Objective:**\n",
    "Perform final cleanup logic to prepare the dataset for modeling and database storage.\n",
    "\n",
    "**Processing Steps:**\n",
    "\n",
    "### 1. Duplicate Handling\n",
    "Aggregate duplicate seasonal entries (e.g., Kharif and Rabi) by summing:\n",
    "\n",
    "- `Area`\n",
    "- `Production`\n",
    "\n",
    "### 2. Case-Sensitivity Fix\n",
    "Ensure perfect joins by creating uppercase matching keys for `State`.\n",
    "\n",
    "### 3. Missing Data Removal\n",
    "Drop rows with `NaN` rainfall values to create an ML-ready dataset.\n",
    "\n",
    "### Mathematical Logic\n",
    "\n",
    "Final Yield is computed as:\n",
    "\n",
    "$Yield_{Final} = \\frac{\\sum Production}{\\sum Area}$\n",
    "\n",
    "**Output:**\n",
    "`Final_Engineered_Dataset.csv` (12,426 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "151228ff999ca61d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T16:54:50.792779Z",
     "start_time": "2026-02-08T16:54:45.772530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STEP 5: FINAL ENGINEERING ---\n",
      "Aggregating Seasonal Data...\n",
      "Filtering valid training data...\n",
      "------------------------------\n",
      "Original Aggregated Rows: 17674\n",
      "Final ML-Ready Rows:      12426\n",
      "------------------------------\n",
      "SUCCESS! Final ML Dataset saved to:\n",
      "rain_fall/results/Final_Engineered_Dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "folder_path = r\"rain_fall/results\"\n",
    "input_file = os.path.join(folder_path, \"Final_Merged_Dataset_Clean.xlsx\")\n",
    "output_file = os.path.join(folder_path, \"Final_Engineered_Dataset.csv\")\n",
    "\n",
    "print(\"--- STEP 5: FINAL ENGINEERING ---\")\n",
    "\n",
    "# 1. Load the Merged Data from Step 9\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# 2. AGGREGATE DUPLICATES (Seasonal Data)\n",
    "print(\"Aggregating Seasonal Data...\")\n",
    "# We group by State, District, Year, and Crop\n",
    "# We SUM Area and Production to get Annual totals\n",
    "df_agg = df.groupby(['State', 'District', 'Year', 'Crop'], as_index=False)[['Area', 'Production']].sum()\n",
    "\n",
    "# Recalculate Yield based on Annual Totals\n",
    "# Avoid Division by Zero\n",
    "df_agg['Yield'] = df_agg.apply(lambda row: row['Production'] / row['Area'] if row['Area'] > 0 else 0, axis=1)\n",
    "\n",
    "# 3. RE-MERGE RAINFALL (To ensure no data loss during grouping)\n",
    "# We need to grab the rainfall column again because the groupby might have dropped it if it wasn't numeric\n",
    "# Or we can just take the first value since Rainfall is constant for that Year/District\n",
    "rainfall_lookup = df[['State', 'District', 'Year', 'Annual_Rainfall']].drop_duplicates()\n",
    "df_final = pd.merge(df_agg, rainfall_lookup, on=['State', 'District', 'Year'], how='left')\n",
    "\n",
    "# 4. DROP MISSING RAINFALL\n",
    "print(\"Filtering valid training data...\")\n",
    "rows_before = len(df_final)\n",
    "df_final_clean = df_final.dropna(subset=['Annual_Rainfall'])\n",
    "rows_after = len(df_final_clean)\n",
    "\n",
    "# 5. SAVE\n",
    "df_final_clean.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Original Aggregated Rows: {rows_before}\")\n",
    "print(f\"Final ML-Ready Rows:      {rows_after}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"SUCCESS! Final ML Dataset saved to:\\n{output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3221a5cbcfe09e",
   "metadata": {},
   "source": [
    "# Part 5: Final Database Architecture & Analysis\n",
    "\n",
    "# Step 11: Building the Final OLTP Database (Normalization)\n",
    "\n",
    "**Objective:**\n",
    "Store the clean, merged dataset into a relational SQLite database using 3rd Normal Form (3NF) to minimize redundancy.\n",
    "\n",
    "For example, long strings such as `\"Andaman and Nicobar Islands\"` are stored once and referenced using IDs.\n",
    "\n",
    "---\n",
    "\n",
    "## Schema Design\n",
    "\n",
    "### Dimension Tables\n",
    "\n",
    "- **States**\n",
    "  - `StateID`\n",
    "  - `StateName`\n",
    "\n",
    "- **Districts**\n",
    "  - `DistrictID`\n",
    "  - `DistrictName`\n",
    "  - `StateID` (Foreign Key)\n",
    "\n",
    "- **Crops**\n",
    "  - `CropID`\n",
    "  - `CropName`\n",
    "\n",
    "### Fact Table\n",
    "\n",
    "- **Crop_Yield_Facts**\n",
    "  - `FactID`\n",
    "  - `Year`\n",
    "  - `Area`\n",
    "  - `Production`\n",
    "  - `Yield`\n",
    "  - `Rainfall`\n",
    "  - `DistrictID` (Foreign Key)\n",
    "  - `CropID` (Foreign Key)\n",
    "\n",
    "**Output:**\n",
    "`Final_Agri_Weather_OLTP.db`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56a460c55cb5d2ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T16:57:49.449803Z",
     "start_time": "2026-02-08T16:57:49.271334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STEP 11: BUILDING FINAL OLTP DATABASE ---\n",
      "Database Created Successfully!\n",
      "------------------------------\n",
      "States Stored:    28\n",
      "Districts Stored: 246\n",
      "Crops Stored:     53\n",
      "Facts Stored:     12426\n",
      "------------------------------\n",
      "Saved to: rain_fall/results/Final_Agri_Weather_OLTP.db\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "folder_path = r\"rain_fall/results\"\n",
    "csv_file = os.path.join(folder_path, \"Final_Engineered_Dataset.csv\")\n",
    "db_path = os.path.join(folder_path, \"Final_Agri_Weather_OLTP.db\")\n",
    "\n",
    "print(\"--- STEP 11: BUILDING FINAL OLTP DATABASE ---\")\n",
    "\n",
    "# 1. Load the Master Dataset\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# 2. Connect to Database\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 3. Create Dimension Tables (Lookup Tables)\n",
    "\n",
    "# --- A. STATES TABLE ---\n",
    "# Get unique states\n",
    "states = df['State'].unique()\n",
    "df_states = pd.DataFrame(states, columns=['StateName']).sort_values('StateName').reset_index(drop=True)\n",
    "df_states.reset_index(inplace=True)\n",
    "df_states.rename(columns={'index': 'StateID'}, inplace=True)\n",
    "df_states.to_sql('States', conn, if_exists='replace', index=False)\n",
    "\n",
    "# --- B. DISTRICTS TABLE ---\n",
    "# Get unique districts linked to states\n",
    "dist_map = df[['State', 'District']].drop_duplicates().sort_values(['State', 'District'])\n",
    "# Merge to get StateID\n",
    "dist_map = dist_map.merge(df_states, left_on='State', right_on='StateName')\n",
    "df_districts = dist_map[['District', 'StateID']].reset_index(drop=True)\n",
    "df_districts.reset_index(inplace=True)\n",
    "df_districts.rename(columns={'index': 'DistrictID', 'District': 'DistrictName'}, inplace=True)\n",
    "df_districts.to_sql('Districts', conn, if_exists='replace', index=False)\n",
    "\n",
    "# --- C. CROPS TABLE ---\n",
    "# Get unique crops\n",
    "crops = df['Crop'].unique()\n",
    "df_crops = pd.DataFrame(crops, columns=['CropName']).sort_values('CropName').reset_index(drop=True)\n",
    "df_crops.reset_index(inplace=True)\n",
    "df_crops.rename(columns={'index': 'CropID'}, inplace=True)\n",
    "df_crops.to_sql('Crops', conn, if_exists='replace', index=False)\n",
    "\n",
    "# 4. Create Fact Table (Transactional Data)\n",
    "# Replace names with IDs\n",
    "fact_df = df.merge(df_crops, left_on='Crop', right_on='CropName')\n",
    "fact_df = fact_df.merge(df_districts, left_on='District', right_on='DistrictName')\n",
    "\n",
    "# Select columns for the final table\n",
    "final_fact = fact_df[['DistrictID', 'CropID', 'Year', 'Area', 'Production', 'Yield', 'Annual_Rainfall']]\n",
    "final_fact.to_sql('Crop_Yield_Facts', conn, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Database Created Successfully!\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"States Stored:    {len(df_states)}\")\n",
    "print(f\"Districts Stored: {len(df_districts)}\")\n",
    "print(f\"Crops Stored:     {len(df_crops)}\")\n",
    "print(f\"Facts Stored:     {len(final_fact)}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Saved to: {db_path}\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e5ddb32368a56e",
   "metadata": {},
   "source": [
    "# Step 12: Final OLAP Analysis (Business Intelligence)\n",
    "\n",
    "**Objective:**\n",
    "Perform multi-dimensional analysis on the engineered dataset to extract meaningful insights.\n",
    "\n",
    "We simulate an OLAP Cube using Pandas.\n",
    "\n",
    "---\n",
    "\n",
    "## Operations Performed\n",
    "\n",
    "### 1. Roll-Up\n",
    "Aggregate production by `State` to identify the highest producing regions.\n",
    "\n",
    "### 2. Dice\n",
    "Filter for a specific sub-cube, for example:\n",
    "- Crop = Rice\n",
    "- High rainfall years\n",
    "\n",
    "### 3. Slice\n",
    "Isolate a specific year (e.g., 2014) to compare crop performance.\n",
    "\n",
    "### 4. Pivot\n",
    "Create a cross-tabulation of Yield trends over the years.\n",
    "\n",
    "### 5. Correlation Analysis\n",
    "Analyze whether rainfall has a measurable impact on yield.\n",
    "\n",
    "Example question:\n",
    "Does increased rainfall significantly increase crop productivity?\n",
    "\n",
    "---\n",
    "\n",
    "**Final Outcome:**\n",
    "A fully engineered Agriculture + Climate dataset stored in a normalized SQL database and analyzed using OLAP-style multi-dimensional operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dd09373986b6590",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T16:58:01.135730Z",
     "start_time": "2026-02-08T16:58:01.019356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STEP 12: OLAP BUSINESS INTELLIGENCE ---\n",
      "\n",
      "[A] ROLL-UP: Total Production by State (Top 5)\n",
      "State\n",
      "Kerala            5.932168e+10\n",
      "Karnataka         3.972231e+10\n",
      "Tamil Nadu        3.632468e+10\n",
      "Andhra Pradesh    1.649057e+10\n",
      "West Bengal       2.747263e+09\n",
      "Name: Production, dtype: float64\n",
      "\n",
      "[B] DICE: Rice Yield in High Rainfall Zones (>2000mm)\n",
      "Average Rice Yield (All Conditions): nan Tonnes/Ha\n",
      "Average Rice Yield (High Rain):      nan Tonnes/Ha\n",
      ">> Insight: Excessive rain might be damaging rice crops.\n",
      "\n",
      "[C] SLICE: Top 3 Crops by Area in 2014\n",
      "Crop\n",
      "Guar seed    4801934.00\n",
      "Sugarcane    1859561.06\n",
      "Coconut      1642284.00\n",
      "Name: Area, dtype: float64\n",
      "\n",
      "[D] PIVOT: Coconut Yield Trend (Select States)\n",
      "State    Karnataka       Kerala    Tamil Nadu\n",
      "Year                                         \n",
      "2016   7989.577877  6351.872046  12357.866089\n",
      "2017   8235.625545  6401.232011  11239.796569\n",
      "2018   7311.646460  6414.736487  12377.821626\n",
      "2019   8325.006031  5894.120740   9410.338391\n",
      "2020      8.113498  5807.458044  10818.304350\n",
      "\n",
      "[E] GLOBAL CORRELATION: Rainfall vs Yield = 0.0198\n",
      "Note: A low number is expected because different crops have different water needs.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the final data\n",
    "file_path = r\"rain_fall/results/Final_Engineered_Dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"--- STEP 12: OLAP BUSINESS INTELLIGENCE ---\")\n",
    "\n",
    "# --- 1. ROLL-UP (Aggregation) ---\n",
    "# Question: Which State has the highest Total Agricultural Production?\n",
    "print(\"\\n[A] ROLL-UP: Total Production by State (Top 5)\")\n",
    "state_prod = df.groupby('State')['Production'].sum().sort_values(ascending=False).head(5)\n",
    "print(state_prod)\n",
    "\n",
    "# --- 2. DICE (Filter multiple dimensions) ---\n",
    "# Question: How does Rice perform in years with Heavy Rainfall (>2000mm)?\n",
    "print(\"\\n[B] DICE: Rice Yield in High Rainfall Zones (>2000mm)\")\n",
    "high_rain_rice = df[\n",
    "    (df['Crop'] == 'Rice') &\n",
    "    (df['Annual_Rainfall'] > 2000)\n",
    "    ]\n",
    "avg_yield_high = high_rain_rice['Yield'].mean()\n",
    "avg_yield_all = df[df['Crop'] == 'Rice']['Yield'].mean()\n",
    "\n",
    "print(f\"Average Rice Yield (All Conditions): {avg_yield_all:.2f} Tonnes/Ha\")\n",
    "print(f\"Average Rice Yield (High Rain):      {avg_yield_high:.2f} Tonnes/Ha\")\n",
    "if avg_yield_high > avg_yield_all:\n",
    "    print(\">> Insight: Rice thrives in high rainfall areas.\")\n",
    "else:\n",
    "    print(\">> Insight: Excessive rain might be damaging rice crops.\")\n",
    "\n",
    "# --- 3. SLICE (Filter one dimension) ---\n",
    "# Question: What were the top crops in 2014?\n",
    "print(\"\\n[C] SLICE: Top 3 Crops by Area in 2014\")\n",
    "slice_2014 = df[df['Year'] == 2014].groupby('Crop')['Area'].sum().sort_values(ascending=False).head(3)\n",
    "print(slice_2014)\n",
    "\n",
    "# --- 4. PIVOT (Cross-Tabulation) ---\n",
    "# Question: How has the Yield of 'Coconut' changed over the years in different states?\n",
    "print(\"\\n[D] PIVOT: Coconut Yield Trend (Select States)\")\n",
    "coconut_df = df[(df['Crop'] == 'Coconut') & (df['State'].isin(['Kerala', 'Tamil Nadu', 'Karnataka']))]\n",
    "pivot = pd.pivot_table(coconut_df, values='Yield', index='Year', columns='State', aggfunc='mean')\n",
    "print(pivot.tail(5)) # Show last 5 years\n",
    "\n",
    "# --- 5. CORRELATION INSIGHT ---\n",
    "# Question: What is the correlation between Rain and Yield across the whole dataset?\n",
    "corr = df['Annual_Rainfall'].corr(df['Yield'])\n",
    "print(f\"\\n[E] GLOBAL CORRELATION: Rainfall vs Yield = {corr:.4f}\")\n",
    "print(\"Note: A low number is expected because different crops have different water needs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df55052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
